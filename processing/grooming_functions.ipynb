{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for grooming analyses \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from collections import OrderedDict\n",
    "from scipy import signal, stats, interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for interpolation\n",
    "def medfilt_data(values, size=15):\n",
    "    padsize = size+5\n",
    "    vpad = np.pad(values, (padsize, padsize), mode='reflect')\n",
    "    vpadf = signal.medfilt(vpad, kernel_size=size)\n",
    "    return vpadf[padsize:-padsize]\n",
    "\n",
    "def nan_helper(y):\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "def interpolate_data(vals):\n",
    "    nans, ix = nan_helper(vals)\n",
    "    out = np.copy(vals)\n",
    "    try:\n",
    "        out[nans] = np.interp(ix(nans), ix(~nans), vals[~nans])\n",
    "    except ValueError:\n",
    "        out[:] = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_erroneous_points(df, bodyparts):\n",
    "    for bodypart in bodyparts:\n",
    "        errors = df.loc[0:][bodypart + '_error']\n",
    "        for i in range(len(errors)):\n",
    "            if errors[i] > 15:\n",
    "                df.set_value(i, bodypart + '_x', np.nan)\n",
    "                df.set_value(i, bodypart + '_y', np.nan)\n",
    "                df.set_value(i, bodypart + '_z', np.nan)            \n",
    "    return df\n",
    "\n",
    "def normalize_data(df):\n",
    "    bodyparts = np.array(['L1A', 'L1B', 'L1C', 'L1D', 'L1E', \n",
    "                          'L2A', 'L2B', 'L2C', 'L2D', 'L2E', \n",
    "                          'L3A', 'L3B', 'L3C', 'L3D', 'L3E', \n",
    "                          'R1A', 'R1B', 'R1C', 'R1D', 'R1E', \n",
    "                          'R2A', 'R2B', 'R2C', 'R2D', 'R2E',\n",
    "                          'R3A', 'R3B', 'R3C', 'R3D', 'R3E'])\n",
    "    L1C = np.array([df.iloc[0:]['L1C_' + xyz] for xyz in 'xyz']).T\n",
    "    L1D = np.array([df.iloc[0:]['L1D_' + xyz] for xyz in 'xyz']).T\n",
    "    measured_length = np.mean(np.linalg.norm(L1C - L1D, axis=1))\n",
    "    expected_length = 0.4556788699978114 \n",
    "    length_ratio = expected_length / measured_length\n",
    "    \n",
    "    for bodypart in bodyparts:\n",
    "        for coord in 'xyz':\n",
    "            df.iloc[0:][bodypart + '_' + coord] = df.iloc[0:][bodypart + '_' + coord] * length_ratio\n",
    "\n",
    "    return df\n",
    "\n",
    "# parse fly summary spreadsheet\n",
    "def clean_summary(prefix, f_in, f_out):\n",
    "    fname_in = os.path.join(prefix, f_in)\n",
    "    fname_out = os.path.join(prefix, f_out)\n",
    "    with open(fname_in, 'r', encoding='utf-8', errors='ignore') as infile, open(fname_out, 'w') as outfile:\n",
    "        inputs = csv.reader(infile)\n",
    "        output = csv.writer(outfile)\n",
    "\n",
    "        for index, row in enumerate(inputs):\n",
    "            output.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_contains(v, L):\n",
    "    for name in L:\n",
    "        if name in v:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_kde_vals(arr, xvals=None):\n",
    "    \n",
    "    x = arr[~np.isnan(arr)]\n",
    "    kde = stats.gaussian_kde(x, bw_method='scott')\n",
    "    \n",
    "    if xvals is None:\n",
    "        xvals = np.linspace(np.min(x)-1, np.max(x)+1)\n",
    "    yvals = kde.evaluate(xvals)\n",
    "    \n",
    "    return xvals, yvals\n",
    "\n",
    "# returns dictionary that maps bout number to fly id\n",
    "def get_fly_id(data, bout_numbers):\n",
    "    fly_id = dict()\n",
    "    for j in range(len(bout_numbers)):\n",
    "        bout_df = data[data.behavior_bout == bout_numbers[j]]\n",
    "        fly_id[bout_numbers[j]] = bout_df.iloc[0].flyid\n",
    "    return fly_id\n",
    "\n",
    "# determine what video corresponds to each bout\n",
    "def get_videos(bout_numbers, labels_df):\n",
    "    videos = dict()\n",
    "    for i in range(len(bout_numbers)):\n",
    "        bout_labels_df = labels_df[labels_df.behavior_bout == bout_numbers[i]]\n",
    "        videos[bout_numbers[i]] = bout_labels_df.iloc[0].filename\n",
    "    return videos\n",
    "\n",
    "# lists fly number (and day)\n",
    "def get_flies(fly_ids): \n",
    "    flies = dict()\n",
    "    dif_flies = np.unique(list(fly_ids.values()))\n",
    "    for i in range(len(dif_flies)):\n",
    "        flies[dif_flies[i]] = i\n",
    "    return flies \n",
    "\n",
    "# get the angle names to analyze (use _BC instead of _abduct for now)\n",
    "def get_angle_names(angles, angle_types, only_t1=False):\n",
    "    angle_names = np.array([])\n",
    "    for ang in angle_types:\n",
    "        if only_t1:\n",
    "            angle_names = np.append(angle_names, [s for s in list(angles.columns) if '1' in s and ang in s])\n",
    "        else:\n",
    "            angle_names = np.append(angle_names, [s for s in list(angles.columns) if ang in s])\n",
    "    # angle_names = angle_names + ['fictrac_speed', 'fictrac_rot']\n",
    "    return angle_names\n",
    "\n",
    "# adjust _rot angles so there are no discontinuities\n",
    "#def adjust_rot_angles(angles, angle_names):\n",
    "#    rot_angs = [r for r in angle_names if '_rot' in r or '_abduct' in r]\n",
    "#    for ang in rot_angs:\n",
    "#        r = np.array(angles[ang])\n",
    "#       r[r > 50] = r[r > 50] - 360\n",
    "#        angles[ang] = r\n",
    "#    return angles\n",
    "\n",
    "def adjust_rot_angles(angles, angle_names):\n",
    "    conds = ['2', '3', 'L1A', 'L1B', 'L1C', 'R1A', 'R1B', 'R1C']\n",
    "    offsets = np.array([-50, -20, 20, -70, 10, 20, 70, -30])\n",
    "    for j in range(len(conds)):\n",
    "        rot_angs = [r for r in angle_names if '_rot' in r and conds[j] in r]\n",
    "        for ang in rot_angs:\n",
    "            r = np.array(angles[ang])\n",
    "            r[r > offsets[j]] = r[r > offsets[j]] - 360\n",
    "            angles[ang] = r\n",
    "        \n",
    "    abduct_angs = [r for r in angle_names if '_abduct' in r or 'A_flex' in r]\n",
    "    for ang in abduct_angs:\n",
    "        r = np.array(angles[ang])\n",
    "        r[r > 50] = r[r > 50] - 360\n",
    "        angles[ang] = r\n",
    "        \n",
    "    return angles\n",
    "        \n",
    "def correct_angles(data, angle_names):   \n",
    "    c_flex_angs = [r for r in angle_names if 'C_flex' in r]\n",
    "    for ang in c_flex_angs:\n",
    "        data[ang] *= np.sign(data[ang])\n",
    "        \n",
    "    legs = ['L1', 'L2', 'L3', 'R1', 'R2', 'R3'] \n",
    "    for leg in legs:\n",
    "        flex = np.array(data['{}A_abduct'.format(leg)])\n",
    "        data['{}A_abduct'.format(leg)] = np.array(data['{}A_flex'.format(leg)])\n",
    "        data['{}A_flex'.format(leg)] = flex\n",
    "        \n",
    "    return data\n",
    "\n",
    "# removes grooming bouts from dataset that are less than a specified \n",
    "# number of frames (too short to analyze)\n",
    "def remove_short_bouts(data, min_frames):    \n",
    "    bout_numbers = np.unique(data.behavior_bout)\n",
    "    bout_lengths = np.zeros(bout_numbers.shape)\n",
    "    for j in range(len(bout_numbers)):\n",
    "        bout_lengths[j] = len(data[data.behavior_bout == bout_numbers[j]])\n",
    "        \n",
    "    saved_bouts = bout_numbers[bout_lengths >= min_frames]\n",
    "    data_new = data[data.behavior_bout.isin(saved_bouts)]  \n",
    "    return data_new\n",
    "\n",
    "# return dict mapping bout number to bout lengths\n",
    "def get_bout_lengths(data):\n",
    "    bout_numbers = np.unique(data.behavior_bout).astype(int)\n",
    "    bout_length_dict = dict()\n",
    "    for j in range(len(bout_numbers)):\n",
    "        bout_length_dict[bout_numbers[j]] = len(data[data.behavior_bout == bout_numbers[j]])\n",
    "    return bout_length_dict\n",
    "\n",
    "# assign a unique bout number to each bout (previously had duplicates due\n",
    "# to running experiments on different days)\n",
    "def adjust_bout_numbers(data):\n",
    "    \n",
    "    dates = np.unique(data.date)\n",
    "    cumulative_bouts = 1\n",
    "    data_new = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(dates)):\n",
    "        \n",
    "        subset = data[data['date'] == dates[i]]\n",
    "        bout_numbers = np.unique(subset.behavior_bout)\n",
    "        bout_numbers_new = np.arange(cumulative_bouts, cumulative_bouts + len(bout_numbers), 1)\n",
    "        cumulative_bouts = cumulative_bouts + len(bout_numbers)\n",
    "        \n",
    "        for j in range(len(bout_numbers)):\n",
    "            subset['behavior_bout'].replace({bout_numbers[j]:bout_numbers_new[j]}, inplace=True)\n",
    "            \n",
    "        data_new = pd.concat([data_new, subset])\n",
    "    \n",
    "    return data_new\n",
    "\n",
    "# determine which flies we have the most data for, then sort by flies with the most data\n",
    "def data_per_fly(data):\n",
    "    bout_numbers = np.unique(data.behavior_bout)\n",
    "    fly_ids = get_fly_id(data, bout_numbers)\n",
    "    fly_data = dict()\n",
    "    for j in range(len(bout_numbers)):\n",
    "        fly = fly_ids[bout_numbers[j]]\n",
    "        bout_length = len(data[data.behavior_bout == bout_numbers[j]])\n",
    "        if fly not in fly_data:\n",
    "            fly_data[fly] = 0\n",
    "        fly_data[fly] += bout_length       \n",
    "    fly_names_sorted = sorted(fly_data, key=fly_data.get, reverse=True) \n",
    "    return fly_data, fly_names_sorted\n",
    "\n",
    "# get all videos of t1 grooming from a given fly\n",
    "def fly_to_video(data):\n",
    "    fly_names = np.unique(data.flyid)\n",
    "    fly_videos = dict()\n",
    "    for j in range(len(fly_names)):\n",
    "        fly_videos[fly_names[j]] = np.unique(data[data.flyid == fly_names[j]].filename)\n",
    "    return fly_videos\n",
    "\n",
    "# assign a different color to each fly\n",
    "def get_fly_colors(fly_ids, colors):\n",
    "    flies = fxn.get_flies(fly_ids)\n",
    "    fly_colors = []\n",
    "    for i in range(len(fly_ids)):\n",
    "        f_id = fly_ids[i+1]\n",
    "        fly_num = flies[f_id]\n",
    "        fly_colors.append(colors[fly_num])\n",
    "    return fly_colors\n",
    "\n",
    "def ymax_coords_corr(df, legs, joints, coords):     \n",
    "    ymin = 0\n",
    "    ymax = 0\n",
    "    for ii in range(len(coords)):\n",
    "        for i in range(len(joints)):                      \n",
    "            t1_l = normalize_corr(df.iloc[0:][legs[0] + joints[i] + coords[ii]])\n",
    "            t1_r = normalize_corr(df.iloc[0:][legs[1] + joints[i] + coords[ii]])\n",
    "            corr = signal.correlate(t1_l, t1_r)\n",
    "            if np.amax(corr) > ymax: \n",
    "                ymax = np.amax(corr)\n",
    "            if np.amin(corr) < ymin:\n",
    "                ymin = np.amin(corr)           \n",
    "    return ymin, ymax\n",
    "\n",
    "def ymax_angles_corr(legs, joints, df):     \n",
    "    ymin = 0\n",
    "    ymax = 0\n",
    "    for i in range(len(joints)):                      \n",
    "        t1_l = normalize_corr(df.iloc[0:][legs[0] + joints[i]])\n",
    "        t1_r = normalize_corr(df.iloc[0:][legs[1] + joints[i]])\n",
    "        corr = signal.correlate(t1_l, t1_r)\n",
    "        if np.amax(corr) > ymax: \n",
    "            ymax = np.amax(corr)\n",
    "        if np.amin(corr) < ymin:\n",
    "            ymin = np.amin(corr)           \n",
    "    return ymin, ymax\n",
    "    \n",
    "# normalizing data for cross correlation\n",
    "def normalize_corr(data):\n",
    "    data = np.array(data)\n",
    "    data_norm = (data - np.mean(data)) / np.std(data)\n",
    "    return data_norm  \n",
    "\n",
    "# given a leg and a joint angle, computes the mean angle for each bout\n",
    "# mean_angles = mean_joint_angles('L1', 'CF', bout_numbers, df_t1_angles, x)\n",
    "def mean_joint_angles(leg, joint, bout_numbers, df_t1_angles, labels_df):   \n",
    "    mean_angles = dict()\n",
    "    for i in range(len(bout_numbers)):\n",
    "        t1_bout_df = df_t1_angles[labels_df.behavior_bout == bout_numbers[i]]\n",
    "        t1 = t1_bout_df.iloc[0:][leg + '_' + joint]\n",
    "        mean_angles[i+1] = np.mean(t1)   \n",
    "    return mean_angles\n",
    "      \n",
    "# given a leg, joint, and coordinate, computes the mean position for each bout \n",
    "# mean_coords = mean_joint_coords('L1', 'A', 'x', bout_numbers, df_t1, x)\n",
    "def mean_joint_coords(leg, joint, coord, bout_numbers, df_t1, labels_df):\n",
    "    mean_coords = dict()\n",
    "    for i in range(len(bout_numbers)):\n",
    "        t1_bout_df = df_t1[labels_df.behavior_bout == bout_numbers[i]]\n",
    "        t1 = t1_bout_df.iloc[0:][leg + joint + '_' + coord]\n",
    "        mean_coords[i+1] = np.mean(t1)      \n",
    "    return mean_coords\n",
    "\n",
    "# offset from origin (0)\n",
    "def origin_offset(leg, joint, coord, bout_numbers, df_t1, labels_df):\n",
    "    joint_offsets = dict()\n",
    "    for i in range(len(bout_numbers)):\n",
    "        t1_bout_df = df_t1[labels_df.behavior_bout == bout_numbers[i]]\n",
    "        t1 = np.array(t1_bout_df.iloc[0:][leg + joint + '_' + coord])     \n",
    "        t1 = t1[~np.isnan(t1)]\n",
    "        joint_offsets[i+1] = t1[0]   \n",
    "    return joint_offsets\n",
    "\n",
    "# function to find peaks of time series data and calculate mean time between peaks\n",
    "# (can find mean interval for troughs if the negative of the data is passed in)\n",
    "def mean_peak_interval(data, fps, thresh = None, dist = None):\n",
    "    data = data[np.isfinite(data)]\n",
    "    idxs, props = signal.find_peaks(data, height = thresh, distance = dist)\n",
    "    peaks = data[idxs]\n",
    "    intervals = np.diff(idxs) / fps # in seconds\n",
    "    mean_interval = np.nanmean(intervals) \n",
    "    stderr_interval = np.nanstd(intervals) / np.sqrt(len(intervals))\n",
    "    return mean_interval, stderr_interval, intervals\n",
    "\n",
    "def get_envelope(data, dist = None, upper = True):\n",
    "    \n",
    "    if not upper:\n",
    "        upper = -1\n",
    "    \n",
    "    env = np.zeros(data.shape) \n",
    "    indices = signal.find_peaks(upper*data, distance = dist)[0]\n",
    "    idxs = np.insert(indices, 0, 0)\n",
    "    peaks = data[idxs[1:]]\n",
    "    peaks = np.insert(peaks, 0, data[0])\n",
    "    idxs = np.append(idxs, len(data)-1)\n",
    "    peaks = np.append(peaks, data[-1])\n",
    "    spline = interpolate.interp1d(idxs, peaks, kind = 'cubic', bounds_error = False, fill_value=0.0)\n",
    "    \n",
    "    for k in range(0,len(data)):\n",
    "        env[k] = spline(k)\n",
    "        \n",
    "    return env, indices[0], indices[-1]\n",
    "\n",
    "def get_envelope_alt(data, dist = None, upper = True):\n",
    "    \n",
    "    if not upper:\n",
    "        upper = -1\n",
    "    \n",
    "    env = np.zeros(data.shape) \n",
    "    indices = signal.find_peaks(upper*data, distance = dist)[0]\n",
    "    if len(indices) > 3:\n",
    "        idxs = np.insert(indices, 0, 0)\n",
    "        peaks = data[idxs[1:]]\n",
    "        peaks = np.insert(peaks, 0, data[0])\n",
    "        idxs = np.append(idxs, len(data)-1)\n",
    "        peaks = np.append(peaks, data[-1])\n",
    "        spline = interpolate.interp1d(idxs, peaks, kind = 'cubic', bounds_error = False, fill_value=0.0)\n",
    "        \n",
    "        for k in range(0,len(data)):\n",
    "            env[k] = spline(k)\n",
    "        \n",
    "    return env, indices\n",
    "\n",
    "def get_leg_joints(legs, joints):\n",
    "    leg_joints = []\n",
    "    for i in legs:\n",
    "        for j in joints:\n",
    "            leg_joint = i + j\n",
    "            leg_joints.append(leg_joint)       \n",
    "    return leg_joints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range(data, angle_vars):\n",
    "\n",
    "    bout_numbers = np.unique(data.behavior_bout.astype(int))\n",
    "    for j in range(len(angle_vars)):\n",
    "        row = []\n",
    "        for i in range(len(bout_numbers)):\n",
    "            bout_data = data[data.behavior_bout == bout_numbers[i]]\n",
    "            bout_angles = np.array(bout_data[angle_vars[j]])\n",
    "            max_peak = np.max(bout_angles)\n",
    "            min_trough = np.min(bout_angles)\n",
    "            avg_amp = np.abs(max_peak - min_trough)\n",
    "            row.extend([avg_amp]*len(bout_angles))\n",
    "        data[angle_vars[j] + '_range'] = row\n",
    "    \n",
    "    return data\n",
    "            \n",
    "# detect peaks and troughs to find the average amplitude\n",
    "def get_average_range(data, ang_vars, dist = 20, height = None):\n",
    "    \n",
    "    bout_numbers = np.unique(data.behavior_bout.astype(int))\n",
    "    for j in range(len(ang_vars)):\n",
    "        row = []\n",
    "        for i in range(len(bout_numbers)):\n",
    "            bout_data = data[data.behavior_bout == bout_numbers[i]]\n",
    "            bout_angles = np.array(bout_data[ang_vars[j]])\n",
    "            peak_idxs, props = signal.find_peaks(bout_angles, distance = dist, height = height)\n",
    "            peaks = bout_angles[peak_idxs]\n",
    "            trough_idxs, props = signal.find_peaks(-1*bout_angles, distance = dist, height = height)\n",
    "            troughs = bout_angles[trough_idxs]\n",
    "            avg_amp = np.abs(np.nanmean(peaks) - np.nanmean(troughs))\n",
    "            row.extend([avg_amp]*len(bout_angles))\n",
    "        data[ang_vars[j] + '_avg_range'] = row\n",
    "    \n",
    "    return data\n",
    "            \n",
    "def get_bout_features(data, feature_names, flip, normalize = True):\n",
    "    bout_numbers = np.unique(data.behavior_bout.astype(int))\n",
    "    bout_features = np.zeros([len(bout_numbers), len(feature_names)])\n",
    "    bout_data = data.groupby(['behavior_bout']).mean()\n",
    "    for i in range(len(bout_numbers)):\n",
    "        bout = bout_data[bout_data.index == bout_numbers[i]]\n",
    "        features = []\n",
    "        for j in range(len(feature_names)):\n",
    "            f = bout[feature_names[j]]\n",
    "            if flip[j]:\n",
    "                f = -1*f\n",
    "            features.append(f)\n",
    "        bout_features[i, :] = features\n",
    "        \n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        bout_features = scaler.fit_transform(bout_features)\n",
    "        \n",
    "    return bout_numbers, bout_features\n",
    "\n",
    "def compute_thresh(lower_scores, higher_scores):\n",
    "    dif = abs(np.min(higher_scores) - np.max(lower_scores)) / 2\n",
    "    thresh = dif + np.max(lower_scores) \n",
    "    return thresh\n",
    "\n",
    "# runs all the steps\n",
    "def compute_grooming_scores(data, angle_vars, features, flip, dist=20, norm=False):\n",
    "    data = get_range(data, angle_vars)\n",
    "    data = get_average_range(data, angle_vars, dist = dist, height = None)\n",
    "    bout_numbers, all_features = get_bout_features(data, features, flip, normalize = norm)\n",
    "    all_scores = np.nanmean(all_features, axis = 1)\n",
    "    data['grooming_score'] = np.nan\n",
    "    for j in range(len(bout_numbers)):\n",
    "        data.loc[(data.behavior_bout == bout_numbers[j]),'grooming_score'] = all_scores[j]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_axes_lim(df_angles, angle_names):\n",
    "    xmin = 180\n",
    "    xmax = 0\n",
    "    for j in range(len(angle_names)):\n",
    "        t1 = df_angles.iloc[0:][angle_names[j]]\n",
    "        t1_min = np.percentile(t1, 5)\n",
    "        t1_max = np.percentile(t1, 95)\n",
    "        if t1_min < xmin:\n",
    "            xmin = t1_min\n",
    "        if t1_max > xmax:\n",
    "            xmax = t1_max    \n",
    "    return np.array([xmin, xmax])\n",
    "\n",
    "def y_lim_dif(df, x, coords, bout_num, joints_1, joints_2):      \n",
    "    ymax = 0\n",
    "    ymin = 0\n",
    "    t1_bout_df = df[x.behavior_bout == bout_num]\n",
    "    for j in range(len(coords)):\n",
    "        \n",
    "        t1_1 = np.array(t1_bout_df.iloc[0:][joints_1[j]])\n",
    "        t1_2 = np.array(t1_bout_df.iloc[0:][joints_2[j]])\n",
    "        t1_diff = t1_1 - t1_2\n",
    "    \n",
    "        if max(t1_diff) > ymax:\n",
    "            ymax = max(t1_diff)\n",
    "        if min(t1_diff) < ymin: \n",
    "            ymin = min(t1_diff)\n",
    "            \n",
    "    return ymin, ymax\n",
    "\n",
    "def adjust_color(color, amount=0.5):\n",
    "\n",
    "    import matplotlib.colors as mc\n",
    "    import colorsys\n",
    "    try:\n",
    "        c = mc.cnames[color]\n",
    "    except:\n",
    "        c = color\n",
    "    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n",
    "    return colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])\n",
    "\n",
    "def custom_cmap(n_colors):\n",
    "    cmap = plt.get_cmap('Spectral')\n",
    "    colors = [cmap(i/(n_colors-1.9999)) for i in range(n_colors)]\n",
    "    colors[4] = 'y'\n",
    "    colors[5] = adjust_color('y', 1.7)\n",
    "    colors[6] = 'g'\n",
    "    colors[7] = adjust_color('g', 1.3)\n",
    "    colors[-1] = adjust_color('#5636a7', 1.3)\n",
    "    return colors\n",
    "\n",
    "def traj_style(ax):\n",
    "    ax.yaxis._axinfo[\"grid\"]['linewidth'] = 0.8\n",
    "    ax.yaxis._axinfo[\"grid\"]['color'] = 'k'\n",
    "    ax.yaxis._axinfo[\"grid\"]['linestyle'] = ':'\n",
    "    ax.xaxis._axinfo[\"grid\"]['linewidth'] = 0.8\n",
    "    ax.xaxis._axinfo[\"grid\"]['color'] = 'k'\n",
    "    ax.xaxis._axinfo[\"grid\"]['linestyle'] = ':'\n",
    "    ax.zaxis._axinfo[\"grid\"]['linewidth'] = 0.8\n",
    "    ax.zaxis._axinfo[\"grid\"]['color'] = 'k'\n",
    "    ax.zaxis._axinfo[\"grid\"]['linestyle'] = ':'\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
